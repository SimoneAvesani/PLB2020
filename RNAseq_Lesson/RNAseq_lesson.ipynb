{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thermal-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "subprocess.call(\"featureCounts\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-collaboration",
   "metadata": {},
   "source": [
    "# Sequencing \n",
    "\n",
    "Genomic sequencing allows to determine the exact nucleic acid sequence contained in our cells.\n",
    "Today, the knowledge of genomic sequences is the basis of almost all biological and medical research fields like \n",
    "Molecular Biology, Virology, Evolutionary Biology and Forensics.  \n",
    "\n",
    "The sequencing technology evolves very quickly leading to a constant improvement in performance and an ever greater reduction in costs.  \n",
    "\n",
    "![](./images/mooreLaw.png)\n",
    "\n",
    "\n",
    "\n",
    "At today, there exist a lot of different companies specialized in sales and development of sequencing technologies, the most famous are:\n",
    "\n",
    "- [Illumina](https://www.illumina.com/)\n",
    "- [QIAGEN](https://www.qiagen.com/it/)\n",
    "- [10X Genomics](https://www.10xgenomics.com/)\n",
    "- [Oxford Nanopore](https://nanoporetech.com/)\n",
    "\n",
    "Each company developed different sequencing pipelines but in general we can say that the modern sequencing methods (Next Generation Sequencing) can be summarized in three main steps:\n",
    "\n",
    "- Library preparation;\n",
    "- Amplification;\n",
    "- Sequencing reaction  \n",
    "\n",
    "### Library preparation \n",
    "The DNA sample is prepared by a process of random fragmentation. The fragments (usually called reads) obtained are added to preset sequences,known as <strong>adapters</strong>, that are necessary to anchor and immobilize the DNA to a support on which the sequencing reaction will take place.  \n",
    "DNA fragments prepared by the addition of adapters constitute the so-called sequencing library.\n",
    "\n",
    "### Amplification \n",
    "\n",
    "sequencing library fragments are incorporated into a microscopic bubble of water together with the so-called enrichment beads (small balls to which adapters can bind). Amplification reaction (PCR) takes place in these aqueous microbubbles, where genomic fragments are amplified many times.\n",
    "\n",
    "### Sequencing reaction  \n",
    "Sequencing reaction takes place thanks to extremely complex mechanisms running on a microlitric scale.\n",
    "\n",
    "\n",
    "## Sequencing approaches\n",
    "Sequencing can be done following two different approaches:  \n",
    "\n",
    "- single-end \n",
    "- paired-end\n",
    "\n",
    "### Single-end\n",
    "The sequencer reads each fragment from only one end to the other generating one sequence of bases.\n",
    "This approach is less accurate but also less expensive respect to the paired-end approach.\n",
    "\n",
    "### Paired-end\n",
    "The sequencer starts to read the fragment, finishes this direction at a specified read length and then starts another round of reading from the opposite end of the fragment.  \n",
    "This approach improves the degree of accuracy in the identification of insertions, deletions, inversion and structural rearrangements.\n",
    "However, this approach is more expensive than the single-end one and also this degree of accuracy may not be required for all the experiments.  \n",
    "\n",
    "<strong>NB</strong>:Today we will work with paired-end reads.\n",
    "\n",
    "\n",
    "![](./images/singlevspair_end.png)## Sequencing output (Fastq)\n",
    "Sequencing output is a set of files (usually one for each sample sequenced) in Fastq format.\n",
    "This format universally used to represent raw sequencing data in the bioinformatics community.\n",
    "Each file contains the set of reads (genomic fragments generated during the library preparation) representing the genome of the considered sample.\n",
    "Fastq file consists of four lines for each read:\n",
    "\n",
    "- <strong>line 1</strong>: begins with a ‘@’ character and is followed by a sequence identifier and\n",
    "an optional description;\n",
    "- <strong>line 2</strong>: is the sequence of the read itself;\n",
    "- <strong>line 3</strong>: begins with a ‘+’ character and is optionally followed by the same sequence\n",
    "identifierr (and any description) again;\n",
    "- <strong>line 4</strong>: is the quality score of the bases in the second line, it must contain the\n",
    "same number of symbols as letters in the sequence;\n",
    "\n",
    "\n",
    "![](./images/fastq.png)\n",
    "\n",
    "## Experimental Data\n",
    "Bioinformaticians work starts now after the acquisition of raw data (Fastq files).\n",
    "In our experiment we have 33 different samples: \n",
    "\n",
    "- 10 healthy controls \n",
    "- 11 mild cognitive impairment subjects (MCI)\n",
    "- 12 Alzheimer's disease patients (AD)\n",
    "\n",
    "For further information about the samples you can check the introduction [slides](./mRNA_lesson.pdf).  \n",
    "\n",
    "The samples were sequenced with the paired-end sequencing approach so, at the end, we have two Fastq files for each sample, one for each reading direction of the reads.\n",
    "In total we have 66 Fastq files each of then of size 3GB (more or less).  \n",
    "In order to reduce the computational effort and time we will try the most expensive steps of the analysis only with a subset of the original samples.  \n",
    "\n",
    "## Quality Control\n",
    "The first step of the bioinformatics analysis is to check the sequencing results and make a quality control analysis of the reads obtained.  \n",
    "FastQC is one of the most used tool for this specific step of the analysis. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.  \n",
    "FastQC tool takes in input different type of data including Fastq files and creates an HTML permanent report which contains the results of the quality control analysis (summary graphs and tables) providing a quick overview and showing you in which reads' areas there may be problems. ([FastQC's website](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))  \n",
    "\n",
    "Below, you can find the python code to generate the FastQC report for each fastq file in the ~/Data/Fastq/Fastq_start directory  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electoral-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simone/Documents/PhD/RNAseq_Lesson\n"
     ]
    }
   ],
   "source": [
    "import os   #import os library useful to run bash command\n",
    "import subprocess\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "diverse-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simone/Documents/PhD/RNAseq_Lesson/Analysis/Reports/FastQC_beforeTrimming/T308\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fastqc /home/simone/Documents/PhD/RNAseq_Lesson/Analysis/Data/Fastq_start/25_combined/T308_combined_R2.fastq.gz --outdir /home/simone/Documents/PhD/RNAseq_Lesson/Analysis/Reports/FastQC_beforeTrimming/T308'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c2d8563ee5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msub_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fastqc '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' --outdir '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#run FastQC on each fastq in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    852\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1700\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fastqc /home/simone/Documents/PhD/RNAseq_Lesson/Analysis/Data/Fastq_start/25_combined/T308_combined_R2.fastq.gz --outdir /home/simone/Documents/PhD/RNAseq_Lesson/Analysis/Reports/FastQC_beforeTrimming/T308'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cwd = os.getcwd()\n",
    "input_dir = cwd + \"/Analysis/Data/Fastq_start/\"\n",
    "out_dir = cwd + \"/Analysis/Reports/FastQC_beforeTrimming/\"\n",
    "\n",
    "for sub_dir in os.listdir(input_dir):\n",
    "    for file in os.listdir(input_dir+sub_dir):\n",
    "    \n",
    "      if not os.path.exists(out_dir+file[0:4]):\n",
    "        os.makedirs(out_dir+file[0:4])         #create a directory for each sample in the out_dir\n",
    "\n",
    "      input = input_dir+sub_dir+'/'+file    \n",
    "      print(out_dir+file[0:4])\n",
    "      subprocess.run('fastqc '+ input + ' --outdir ' + out_dir+file[0:4]) #run FastQC on each fastq in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-indian",
   "metadata": {},
   "source": [
    "Here you have two examples of HTML report file for the sample T112:  \n",
    "[FastQC's example read1](./Examples/T112_combined_R1_fastqc.html)  \n",
    "[FastQC's example read2](./Examples/T112_combined_R2_fastqc.html) \n",
    "\n",
    "We have to check if, in our html reports, there are features highlighted in red or yellow.  \n",
    "Here you can find a very quick guide which describe the main features to consider inside the report ([click here](./Extra_material/FastQC.pdf)).  \n",
    "\n",
    "In our samples we can observe that the \"Per base sequence content\" and the \"Sequence Duplication Levels\" are highlighted in red. This is something that we have to take into account but, in general, we can say that the plots show profiles common between the majority of the sequencing.\n",
    "In general we can say that these profiles are due to the high number of reads that we have for each sample (>> 20M) and also to some technical issues but we can also say that it is not necessary to apply corrections in order to mitigate these issues.  \n",
    "\n",
    "However, one aspect that is very important to consider, is the possible presence of the adapters in our reads.\n",
    "If the adapters are still inside our reads we have to remove them to avoid problems in the next steps of the analysis.\n",
    "We can check the presence of the adapters observing in the report if there are repetitive sequences identified as adapters or if the \"Adapter content\" section shows something strange in the curve profile (usually highlighted in red).  \n",
    "In our case we can observe that some of the repetitive sequences are recognized as adapters.\n",
    "\n",
    "To remove the adapters we proceed with the next step of the analysis called Trimming.\n",
    "\n",
    "\n",
    "## Trimming\n",
    "Trimming step is not always necessary.\n",
    "During this step we applied different type of corrections such as removing adapters or not good regions inside the reads, usually generated due to technical problems during the sequencing.  \n",
    "We can perform this pre-processing step with different tools:\n",
    "\n",
    "- Cutadapt\n",
    "- Trimmomatic\n",
    "\n",
    "In this lesson we will use [Trimmomatic](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).  \n",
    "\n",
    "To work, Trimmomatic requires you to have installed a recent version of java. The tool provides a lot of different parameters useful to cut reads' regions and in particular in our experiment we are interested in removing adapters applying ILLUMINACLIP step.  \n",
    "ILLUMINACLIP requires you to specify which adapters to remove, in our case we will use \"TruSeq3-PE-2.fa\" adapters file.  \n",
    "You can find different adapters files to use inside the Trimmomatic tool folder. Usually we have to select the adapters file taking into account if we have paired-end reads or single-end reads and also the file version, choosen considering the specifics of the sequencing (usually the most recently updated file).  \n",
    "\n",
    "\n",
    "Other important parameters are:\n",
    "\n",
    "- PE: to specify that we are working with paired-end reads\n",
    "- -threads: to specify the number of threads to use\n",
    "- -baseout: to specify the the output filenames base (including the file extension)\n",
    "- the two input reads files of each sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "anonymous-advance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing dir: T308\n",
      "Existing dir: T115\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "cwd = os.getcwd()\n",
    "\n",
    "dir = cwd + \"/Analysis/Data/Fastq_start/\"\n",
    "adapters = \"ILLUMINACLIP:/usr/share/trimmomatic/TruSeq3-PE-2.fa:2:30:12\"\n",
    "\n",
    "for sub_dir in os.listdir(dir):\n",
    "    for file in os.listdir(dir+sub_dir):\n",
    "\n",
    "        if file.endswith('R1.fastq.gz'):\n",
    "            R1 = dir + sub_dir + \"/\" + file   #take read1 file\n",
    "        else:\n",
    "            R2 = dir + sub_dir + \"/\" + file   #take read2 file\n",
    "        \n",
    "    name = re.search(\"(.+?)_combined\", file).group(1)    # extract sample name\n",
    "    if os.path.isdir(cwd + \"/Analysis/Data/Fastq_trimmed/\" +name):   #check if there is a sample name directory\n",
    "        print(\"Existing dir: \"+name)\n",
    "    else:\n",
    "        os.mkdir(cwd + \"/Analysis/Data/Fastq_trimmed/\"+name)   #create the sample name directory\n",
    "\n",
    "    out_dir = cwd + \"/Analysis/Data/Fastq_trimmed/\" + name+\"/\"\n",
    "\n",
    "    os.system('TrimmomaticPE -threads 6 ' + R1 + ' ' + R2 + ' -baseout ' + out_dir + name +'.fastq.gz' + ' '+ adapters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-landscape",
   "metadata": {},
   "source": [
    "The numbers specified after the adapters' file show that initially Trimmomatic will look for seed matches (16 bases) allowing maximally 2 mismatches. These seeds will be extended and clipped if in the case of paired end reads a score of 30 is reached, or in the case of single ended reads a score of 12.    \n",
    "  \n",
    "### Trimmomatic output\n",
    "\n",
    "Since we are working with paired-end reads, the tool's output consists of four different files:\n",
    "\n",
    "- mySampleFiltered_1P.fq.gz-for paired forward reads\n",
    "- mySampleFiltered_1U.fq.gz-for unpaired forward reads\n",
    "- mySampleFiltered_2P.fq.gz-for paired reverse reads\n",
    "- mySampleFiltered_2U.fq.gz-for unpaired reverse reads\n",
    "\n",
    "where 'paired' files contain the reads both survived to the processing while 'unpaired' files correspond to the read survived but whose read partner did not.\n",
    "\n",
    "![](./images/Trimmomatic.png)   \n",
    "\n",
    " <strong>NB</strong>: in the next steps of the analysis we will work only with the paired files.  \n",
    " \n",
    " \n",
    " \n",
    "## Second Quality Control \n",
    "\n",
    "To make sure you have removed all the adapters, is always a good choice to use FastQC to check the trimming results.\n",
    "Following you can find an example of FastQC HTML report after adapters removal.    \n",
    "\n",
    "Below you can find an example of HTML report for sample T112 (read1) after trimming:  \n",
    "[FastQC's trimmed example read1](./Examples/T112_1P_fastqc.html) \n",
    "\n",
    "\n",
    "## Alignment\n",
    "\n",
    "\n",
    "As we know, sequencing produces a collection of fragments without genomic context and we don't know to which part of the genome the sequences correspond to.  \n",
    "What we are going to do is to map the reads of each sample to reference genome.   \n",
    "With the mapping, reads are assigned to a specific location in the genome and insights like the expression level of genes can be estimated.\n",
    "\n",
    "Alignment is one of the most challenging topic in Bioinformatics which requires a good compromise between precision and efficiency in term of computational resources (memory, computational time), you will see some of the most famous alignment algorithms in the Algorithms course.  \n",
    "\n",
    "![](./images/Alignment.png)\n",
    "\n",
    "The most famous tools used for this step of the analysis are:\n",
    "\n",
    "- BWA\n",
    "- Bootie\n",
    "- STAR \n",
    "\n",
    "Their efficiency changes respect to the length of the reads that we want to map and also respect to the species reference genome that we want to use.  \n",
    "In our case we will use STAR tool which is very efficient and accurate in mapping long reads (as ours) to the human reference genome.\n",
    "\n",
    "<strong>NB</strong>: STAR requires a lot of RAM, usually for Human genome alignment it needs ~32 GB so pay attention to run it on your PC!  \n",
    "\n",
    "\n",
    "### Reference Genome\n",
    "\n",
    "Human reference genome is a digital genome assembled in laboratory, it doesn’t represent the genetic sequence for any one individual, but is made up of a combination of several people’s DNA. When we sequence a patient’s genome and compare it to the reference genome, we assume that the reference represents the ‘normal’ sequence, and therefore any identified variation could be responsible for problems.  \n",
    "The human reference genome is continuously evolving. Since the Human Genome Project, it has been updated in line with new information and knowledge thanks to the collaboration of scientists around the world, and it is currently in its 38th iteration. \n",
    "Reference genomes are free and can be downloaded from the web.\n",
    "\n",
    "### GFF/GTF File Format\n",
    "\n",
    "Now, we have the reads which are like sentences in a text and we want map them inside a book (reference genome) but our book has no index therefore search a word inside  it could be very expensive in terms of time.  \n",
    "\n",
    "The idea is to index our reference genome before to start the mapping.  \n",
    "To index the reference genome we need a GFF (general feature format) or GTF (general tranfer format) file in which are stored all the information about genomic features.\n",
    "These files consists of one line for each feature, each containing 9 columns of data, plus optional track definition lines.\n",
    "The 9 fields for each feature, tab separated are: \n",
    "\n",
    "- <strong>seqname</strong> - name of the chromosome or scaffold; chromosome names can be given with or without the 'chr' prefix. Important note: the seqname must be one used within Ensembl, i.e. a standard chromosome name or an Ensembl identifier such as a scaffold ID, without any additional content such as species or assembly. See the example GFF output below.\n",
    "- <strong>source</strong> - name of the program that generated this feature, or the data source (database or project name)\n",
    "- <strong>feature</strong> - feature type name, e.g. Gene, Variation, Similarity\n",
    "- <strong>start</strong> - Start position* of the feature, with sequence numbering starting at 1.\n",
    "- <strong>end</strong> - End position* of the feature, with sequence numbering starting at 1.\n",
    "- <strong>score</strong> - A floating point value.\n",
    "- <strong>strand</strong> - defined as + (forward) or - (reverse).\n",
    "- <strong>frame</strong> - One of '0', '1' or '2'. '0' indicates that the first base of the feature is the first base of a codon, '1' that the second base is the first base of a codon, and so on..\n",
    "- <strong>attribute</strong> - A semicolon-separated list of tag-value pairs, providing additional information about each feature.\n",
    "\n",
    "\n",
    "There exists a lot of different GTF/GFF files each of one contains different information necessary for different type of analysis ([Gencode example](https://www.gencodegenes.org/human/)).  \n",
    "\n",
    "### Reference Genome indexing\n",
    "\n",
    "Each of the tools mentioned above allows to index the reference genome, below you find the code to do it with STAR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "cwd = os.getcwd() #return the current path \n",
    "\n",
    "subprocess.call('STAR  --runThreadN 6 --runMode genomeGenerate --genomeDir ' + cwd+ '/Analysis/Data/Genome/Genome_index  --sjdbGTFfile ' + cwd+'/Analysis/Data/Genome/Genome/Homo_sapiens.GRCh38.103.chr.gtf --genomeSAindexNbases 12  --genomeFastaFiles ' + cwd +'/Analysis/Data/Genome/Genome/Homo_sapiens.GRCh38.dna.chromosome.10.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-discretion",
   "metadata": {},
   "source": [
    "The parameters are very simple, the tool requires the fasta file of the reference genome, a GTF or GFF file and the output folder where you will find your indexed genome files.   \n",
    "To speedup the indexing step you can run it in parallel specifying an higher number of threads. In the [STAR manual](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf) you can find the complete list of parameters.  \n",
    "\n",
    "<strong>NB</strong>: in the example, in order to speedup the indexing, we are working only with chromosome 10 of the genome.\n",
    "\n",
    "### Genome alignment\n",
    "\n",
    "After the reference genome indexing we can proceed by mapping the trimmed reads on the genome.  \n",
    "Using STAR we can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dir = cwd + '/Analysis/Data/Fastq_trimmed/'\n",
    "for sample in os.listdir(dir):\n",
    "    sample_dir = dir + sample + '/'\n",
    "    for file in os.listdir(sample_dir):\n",
    "        if file.endswith('1P.fastq.gz'):\n",
    "            R1 = sample_dir + file     #take read1 file\n",
    "        if file.endswith('2P.fastq.gz'):\n",
    "            R2 = sample_dir + file     #take read2 file\n",
    "            \n",
    "    subprocess.call('STAR  --runThreadN 15 --genomeDir ' + cwd+ '/Analysis/Data/Genome/Genome_index --readFilesIn ' + R1 +' ' + R2 +  ' --outFileNamePrefix ' + cwd + '/Analysis/Data/Alignment/T308_  --outSAMtype BAM SortedByCoordinate --outSAMunmapped Within --outSAMattributes Standard --readFilesCommand zcat', shell=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-right",
   "metadata": {},
   "source": [
    "\n",
    "<strong>NB</strong>: if the percentage of uniquely mapped reads is around 85% the alignemnt is fine, otherwise we have to go back and try to check if there are some factors that do not allow a good alignment (high duplication level, adapters not removed ...).  \n",
    "\n",
    "\n",
    "## Alignment output \n",
    "\n",
    "The alignment step could produce different output files with different formats. The most common are:\n",
    "\n",
    "- SAM\n",
    "- BAM \n",
    "\n",
    "### SAM file\n",
    "\n",
    "SAM is a text based format used to store sequences aligned to a reference sequence, it is a TAB-delimited text format consisting of a header section, which is optional, and an alignment section.  \n",
    "If present, the header must be prior to the alignments. (header lines start with ‘@’, while alignment lines do not).  \n",
    "Each alignment line has 11 mandatory fields for essential alignment information such as mapping position, and\n",
    "variable number of optional fields for flexible or aligner's specific information.\n",
    "\n",
    "### BAM file\n",
    "\n",
    "BAM file is compressed binary representation of the SAM file and it contains exactly the same information.  \n",
    "The only difference between the two type of extension is that SAM is a human-readable format while BAM is only machine readable.\n",
    "\n",
    "\n",
    "## SAMTools\n",
    "\n",
    "SAMtools provides various utilities for manipulating alignment files like SAM and BAM formats.  \n",
    "It provides complex tasks such as sorting, indexing, data extraction and format conversion.  \n",
    "Below you can find an example of the tool's functionality in which we sort the reads in a BAM file by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "dir = cwd + '/Analysis/Data/Alignment/'\n",
    "for file in os.listdir(dir):\n",
    "    sample = file[0:4]   #extract sample name\n",
    "    print(sample)\n",
    "    bam= dir+file\n",
    "\n",
    "    subprocess.call('samtools sort -n -o ' + cwd + '/Analysis/Data/Samtools_Sort/' + sample+'.bam'+ ' -@ 4 ' + bam, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-frederick",
   "metadata": {},
   "source": [
    "## Quantification\n",
    "\n",
    "After that we have aligned all the reads to the Human Reference Genome we are ready quantify how many reads have been mapped to coding regions/genes.  \n",
    "The idea is to count how many reads are aligned to a specific gene's sequence for each sample and create a raw count matrix in which each row is a gene, each column is a sample and each value is the count of reads mapped in that specific gene's region for each sample.  \n",
    "\n",
    "The most used tools for this analysis' step are:\n",
    "\n",
    "- HTSeq\n",
    "- featureCounts\n",
    "\n",
    "In this course we will use featureCounts which is part of the Subread package in R.  \n",
    "This tool, also available from command line, takes as input one or more BAM/SAM files and a GTF annotation file and it outputs numbers of reads assigned to features and the overall summarization results creating a raw count matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "professional-collar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "cwd = os.getcwd()\n",
    "subprocess.call('featureCounts -T 5 -p -a '+ cwd + '/Analysis/Data/Genome/GTF_quantification/gencode.v36.primary_assembly.annotation.gtf -o ' + cwd + '/Analysis/Data/Raw_count_matrices/raw_matrix.txt ' + cwd + '/Analysis/Data/Alignment/*.bam', shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-cause",
   "metadata": {},
   "source": [
    "In addition to the input files, GTF file and the output file we have to specify that we are working with paired-end reads using the parameter -p.\n",
    "For a complete overview of all the parameters check the\n",
    "[featureCounts manual](http://www.bioconductor.org/packages/release/bioc/vignettes/Rsubread/inst/doc/SubreadUsersGuide.pdf)\n",
    "\n",
    "<strong>NB</strong>: we have a good quantification result if the successfully assigned alignments are greater than 55%/60% (percentage of reads mapped in exonic regions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
